{{
metadata = {
	title: "Rendering like it's 1996 - Bitmap fonts and DOS",
	summary: "On drawing bitmap fonts and getting all of this running in DOS.",
	image: "dos-nostalgia.png",
	date: parseDate("2022/12/7 21:00"),
	published: false,
}
}}

{{include "../../../_templates/post_header.bt.html"}}
{{include "../../../_templates/post_header.bt.html" as post}}
{{include "../_demo.bt.html" as demo}}

{{post.figure("dos-nostalgia.png", "This screen has burned itself into my retina.")}}

<div class="table_of_contents"></div>

--markdown-begin

[Last time](../blistering-fast-blits/) we learned about loading images and blitting. That was over 2 weeks ago, making me miss my target of posting one series entry a week. But there's a reason for it! I was rather busy in those two weeks.

After using Hopper to generate control flow graphs to discuss performance optimization, I got a little sick of the workflow and built my own [assembly CFG viewer](https://marioslab.io/projects/cfg/). Just paste some x86 or ARM assembly generated by MSVC, Clang, or GCC into the left panel, and view the control flow graph of each function on the right. I also made it a [re-usable NPM package](https://www.npmjs.com/package/@marioslab/asmcfg). Going forward, I can embed those fancy CFGs directly.

Then I drifted off into yet another rabbit hole. Spurred by a mean comment on Reddit about how the `r96` code doesn't even run in DOS, I made the code of the series run in DOS.

First, I built a [DOS backend for MiniFB](https://github.com/emoon/MiniFB/pull/98). Then, I [forked an old GDB version](https://github.com/badlogic/gdb-7.1a-djgpp/releases/tag/gdb-7.1a-djgpp) which is capable of remotely debugging 32-bit protected mode DOS programs as produced by [DJGPP](https://www.delorie.com/djgpp/), the GCC fork I use to build C/C++ DOS programs. I also forked [DOSBox-x](https://github.com/badlogic/dosbox-x/) to fix it up so my forked GDB can actually connect to DOS programs via the serial port/TCP emulation.

Finally, I took the barely functional [GDB stub](https://sourceware.org/gdb/onlinedocs/gdb/Remote-Stub.html) that comes with DJGPP, rewrote it and added a ton of functionality to it, so I can now debug DOS programs running in DOSBox-x from the comforts of Visual Studio Code.

All of that work culminated in a [VS Code extension](https://marketplace.visualstudio.com/items?itemName=badlogicgames.dos-dev), which lets you go from 0 to debugging a simple DOS mode 13h demo app in VS code in about 80 seconds:

<img src="vsc-extension.gif" style="max-width: 100%"></img>

With all of that out of my system, I built some shell scripts that will help you install (almost) all the tools to compile, run, and debug the `r96` project for desktop, web, and DOS. And I added some VS Code magic so you can comfortably start debugging sessions on each platform.

And to round it all off, I cleaned up the Git repo, so each blog post maps to exactly one commit. And I rewrote the first 3 blog posts in the series. So yeah.

I can now happily continue writing the series. Promise. Unless I'll add Android and iOS support in the future. I currently don't feel that specific masochism piling up inside of me.

Today, we're looking DOS support, and then load and draw some bitmap fonts.

## Demo: Hello DOS
Alright, go get the latest and greatest from the `r96` repository. Follow the [README.md](https://github.com/badlogic/r96/blob/04-dos-nostalgia/README.md) to install the tools, including the new DOS tools. The README.md will also get you up to speed on how to build and debug everything in VS Code or on the command line. Or, if you want a detailed run-down of the project and its build and IDE support, read the [first entry of the series](posts/rendering-like-its-1996/babys-first-pixel/).

To celebrate DOS support, I've added a new demo called [`12_hello_dos.c`](https://github.com/badlogic/r96/blob/04-dos-nostalgia/src/12_hello_dos.c):

--markdown-end
{{post.code("12_hello_dos.c", "c",
`
#include <MiniFB.h>
#include <stdio.h>
#include "r96/r96.h"
#include "stdlib.h"
#include <math.h>

#define GDB_IMPLEMENTATION
#include "dos/gdbstub.h"

#define num_grunts 100

typedef struct grunt {
	int x, y, vx, vy;
} grunt;

int main(void) {
	gdb_start();
	r96_image image;
	if (!r96_image_init_from_file(&image, "assets/grunt.png")) {
		printf("Couldn't load file 'assets/grunt.png'\n");
		return -1;
	}

	r96_image output;
	r96_image_init(&output, 320, 240);
	struct mfb_window *window = mfb_open("12_hello_dos", output.width, output.height);

	grunt grunts[num_grunts];
	for (int i = 0; i < num_grunts; i++) {
		grunt *grunt = &grunts[i];
		grunt->x = rand() % 320;
		grunt->y = rand() % 200;
		grunt->vx = 1;
		grunt->vy = 1;
	}
	do {
		r96_clear_with_color(&output, 0xff222222);
		for (int i = 0; i < num_grunts; i++) {
			grunt *grunt = &grunts[i];
			if (grunt->x < 0) {
				grunt->x = 0;
				grunt->vx = -grunt->vx;
			}
			if (grunt->x > 320 - 64) {
				grunt->x = 320 - 64;
				grunt->vx = -grunt->vx;
			}
			if (grunt->y < 0) {
				grunt->y = 0;
				grunt->vy = -grunt->vy;
			}
			if (grunt->y > 240 - 64) {
				grunt->y = 240 - 64;
				grunt->vy = -grunt->vy;
			}
			grunt->x += grunt->vx;
			grunt->y += grunt->vy;
			r96_blit_keyed(&output, &image, grunt->x, grunt->y, 0x00000000);
		}
		if (mfb_update_ex(window, output.pixels, output.width, output.height) != STATE_OK) break;
		gdb_checkpoint();
	} while (mfb_wait_sync(window));

	r96_image_dispose(&image);
	r96_image_dispose(&output);
	return 0;
}
`)}}
--markdown-begin

This is our first animated demo!

The demo draws 100 moving grunts, that bounce off of the screen boundaries. Each grunt is stored in a simple `grunt` struct, which in turn stores the grunt's position (`x`, `y`) and velocity on the x- and y-axis (`vx`, `vy`) in pixels per frame. During initialization, we give each grunt a random position within the screen boundaries and set their velocity on each axis to `1` (lines 29-35).

What's a frame you may ask? A frame can be many things, but in our case, a frame is simply one iteration of the main loop of your program (lines 36-62). In each frame, we check whether each grunt is still inside the screen boundaries. If a grunt is outside the screen boundaries on the x- or y-axis (or both), we move them back inside the bounds and negate their velocity on the axis they left the screen on.

E.g. a grunt moving to the right (`vx = 1`), leaving the screen on the x-axis (`x > 320 - 64`), will be moved back inside the screen boundaries (`x = 320 - 64`), and its velocity on the x-axis will become `-1`. Starting in the next frame, the grunt will then move to the left, until it exits the screen boundaries on the left side of the screen. The same happens on the y-axis.

Once all the checks are complete, we add the grunt's velocity to its position. Each frame, the grunt's position thus changes by `vx` pixels on the x-axis, and `vy` pixels on the y-axis. Hence why `vx` and `vy` are given as pixels per frame.

Now, there's one big problem with this type of moving objects: it depends on the speed of execution.

We call `mfb_wait_sync()`, which waits for a vertical refresh, effectively limiting the number of frames per second to the screen refresh rate, so 60Hz, 90Hz, 120Hz, or whatever other wonky screen refresh rate the display has.

On a 60Hz screen a grunt will thus move 60 pixels per second, on a 120Hz it will move 120 pixels.

For a game, that's not great: different players will experience the game at different speeds, depending on their hardware. We'll look into this issue in a future series entry.

> **Note:** Many old DOS games actually did have this problem: they would not take into account how much time has passed since the last frame, but instead update game object positions at a fixed rate each frame. There's a reason Wikipedia has an entry on the notorious [PC turbo button](https://en.wikipedia.org/wiki/Turbo_button).

Here's the little demo on the web:

--markdown-end
{{demo.r96Demo("12_hello_dos", false)}}
--markdown-begin

And here it is running in DOSBox-x, telling DOSBox-x to go full speed.

--markdown-end
<video style="margin: auto; max-width: 100%;" src="grunts-dosbox.mp4" autoplay=true loop=true controls=true></video>
--markdown-begin

DOSBox-x on my system syncs to 60Hz in windowed mode, while Chrome runs the web demo at the full 120Hz of my display. In the video above, there is some smearing and artifacts. That's mostly due to the MP4 encoding and doesn't look like that when actually running the demo in DOSBox-x on your system.

Is the DOSBox-x performance indicative of performance on old systems? No. DOSBox-x is going full speed, which is way faster than what my old 486 could do. However, you can modify the emulation speed via the DOSBox-x menu `CPU > Emulated CPU speed`. In the following video, I've set the emulated CPU speed to be equivalent to a 486DX2 with 66Mhz:

--markdown-end
<video style="margin: auto; max-width: 100%;" src="grunts-dosbox-486.mp4" autoplay=true loop=true controls=true></video>
--markdown-begin

While that's more accurate, it's still not quite the same as real hardware. To get a more accurate sense of how the program would perform on a real 486, we can use [86Box](https://86box.net/). 86Box is as cycle accurate emulator for various old x86 systems.

--markdown-end
<video style="margin: auto; max-width: 100%;" src="86box.mp4" autoplay=true loop=true controls=true></video>
--markdown-begin

Looks like DOSBox-x isn't far off with its emulation. So why is it so slow?

> **Note:** Setting up virtual machine images for 86Box is a bit terrible. I've created 2 images you can download, a [486](https://marioslab.io/dump/dos/86box/486.zip) image and a [Pentium](https://marioslab.io/dump/dos/86box/pentium.zip) image, pre-installed with MS-DOS 6.22, a mouse driver, and a CD-ROM driver. You can run them via `86box -c 486/86box.conf` and `86box -c pentium/86box.conf`. The images also include QBasic 1.1. And [`NIBBLES.BAS`](https://en.wikipedia.org/wiki/Nibbles_(video_game)) and [`GORILLA.BAS`](https://en.wikipedia.org/wiki/Gorillas_(video_game)). Just saying.

## Why is it so slow on a 486?

The MiniFB DOS backend sets up a video mode with either 24-bit or 32-bit color depth through [VESA](https://en.wikipedia.org/wiki/VESA_BIOS_Extensions). MiniFB assumes 32-bit color depth, so we have to abide by that and go VESA.

This works pretty well from Pentium class machines onwards, if the (emulated) video card supports VESA. Here's the demo on Pentium class hardware in 86Box:

--markdown-end
<video style="margin: auto; max-width: 100%;" src="86box-pentium.mp4" autoplay=true loop=true controls=true></video>
--markdown-begin

A 486 may support 24-bit and 32-bit color depth video modes, depending on the graphics cards. Mine did. However, that doesn't mean the system is fast enough to actually deal with that amount of data. A run of the mill 486 would have memory throughput somewhere in the range of 10-25MB/s. You read that right.

In our demo above, we render to a 320x240 output `r96_image`. The call to `r96_clear_with_color()` has to touch `0.3M` worth of pixels. Rendering a single grunt means reading 64x64x4 bytes from the grunt image and writing them to a 64x64x4 bytes big region in the output `r96_image`. For 100 grunts, that's reading `1.6MB` and writing `1.6MB`. Finally, the output `r96_image` is transferred to the VESA linear buffer, a memory mapped region from which the graphics card will read what it should output to the display. That's another 320x240x4 bytes, or `0.3MB`. Each frame we thus touch `0.3 + 1.6 + 1.6 + 0.3 = 3.8MB` of memory. And while this simple analysis doesn't account for memory caches, it does align with what we experience when running the demo on a (emulated) 486. We do indeed only get something like 3-5 frames per second, which is `11.4-19MB` of data pushed by the demo per second.

That's one of the reasons pretty much all older DOS games targeting 386 or 486 would use [mode 13h](https://en.wikipedia.org/wiki/Mode_13h) or derivatives like [Mode X](https://en.wikipedia.org/wiki/Mode_X). Both of these video modes use 8 bits to encode a pixel's color. But instead of directly encoding the color's red, green and blue component, the 8-bit value is an index into a palette with a total of 256 colors. That cuts down on memory and bandwidth needs considerably.

If we went mode 13h on our demo, we'd go from `3.8MB` to `0.95MB` of data per frame. That translates to 12-20 frames per second, which is still not great, but often playable enough. That's about the frame rate I got when playing [MicroProse's Formula One Grand Prix](https://www.youtube.com/watch?v=qATaCWHLAxw) on my 486.

So what's the solution? Draw less each frame! DOOM and Quake relied on various techniques like [binary space partitioning](https://twobithistory.org/2019/11/06/doom-bsp.html) to avoid drawing things that are invisible or occluded. Drawing less means touching less memory. Consider that 100 grunts are about 5.3 screens worth of pixels. That's a lot of overdraw.

Yes, we could probably squeeze a lot of cycles out of the blitting functions if we handcrafted some 32-bit x86 assembly. But DJGPP actually does a pretty good job at producing fast machine code. And I don't want to drop down into assembly land.

> **Note:** modern hardware won't save you from these issues either sometimes. When NVIDIA sent me a prototype Tegra board in the early 2010s, I soon found out that you could only render about 2 full-screen alpha blended rectangles through OpenGL ES before the frame-rate takes a heavy hit.

## Excursion: DOS debugging support
When we debug the demo on the desktop, the debugger will spawn the demo process and use system APIs to stop, resume, inspect, and otherwise manipulate the process.

For DOS applications running in DOSBox-x or on a real machine, we do not have the luxury of a debugger. Instead, we use a piece of code called [GDB stub](https://sourceware.org/gdb/onlinedocs/gdb/Remote-Stub.html). Here's how that works.

Of note are 3 pieces of code in the demo above, which do nothing on any platform other than DOS. In lines 7-8 we have:

```
#define GDB_IMPLEMENTATION
#include "dos/gdbstub.h"
```

This pulls in [my GDB stub implementation for DJGPP/DOS](https://github.com/badlogic/r96/blob/main/src/dos/gdbstub.h), which is a single header file library.

The stub's task is it to communicate with the debugger over the serial port, and tell it when the program has stopped due to a breakpoint, or segfault, or other reason. The stub then waits for commands from the debugger to execute, like setting breakpoints, inspecting memory and CPU registers, stepping, continuing, etc.

This GDB stub type of debugging is a cooperative debugging approach. The stub needs to be integrated with the program itself. This explains the other two GDB related lines of code in the demo.

The `gdb_start()` function is called at the beginning of `main()`. It waits for the debugger to connect on the serial port. When the debugger tells the stub to continue execution of the program, the stub stops communicating with the debugger for the time being, and gives back control to the program.

The stub then waits for a system level signal to be raised, like a breakpoint or segfault, for which the stub has registered handlers. If such a signal happens, the stub takes over control from the program again, tells the debugger about the program being stopped, and waits for debugger commands to execute.

The final GDB related line is `gdb_checkpoint()` in line 61. It is placed at the end of our main loop. This is required so the stub can check if the debugger asked to interrupt the program, in which case the stub will take control of the program again and talk to the debugger.

The GDB stub expects all communication to happen through serial port `COM1`. Some emulators and virtual machines, like DOSBox-x or VirtualBox, can expose the emulated serial port as a TCP port to programs on the host OS. That's what's happening when we debug a demo in DOSBox-x. DOSBox-x exposes the serial port on TCP port 5123, to which GDB connects via TCP. DOSBox-x will then translate TCP packages to writes to the serial port, which the GDB stub reads from `COM1`. If the GDB stub writes to `COM1`, then DOSBox-x will forward the data through TCP to GDB.

In theory, the GDB stub should also work on real-hardware. Sadly, I do not have my 486 anymore, nor a serial cable or a serial port on my MacBook.

If you want to debug any of the demos in DOS, you'll have to add the 3 pieces of GDB stub related code to the demo's sources as outlined above. Only the `12_hello_dos.c` demo is currently set-up for DOS debugging. Since our code is cross-platform, there won't be a need to debug in DOS a lot though.

> **Note:** when debugging the demos compiled for DOS, we'll be using DOSBox-x instead of 86Box. Two reasons: getting data into and out of 86Box is very annoying. And there is no serial port over TCP emulation in 86Box, so the debugger couldn't even connect. It should be possible to hook the debugger up with a program running in MS-DOS or FreeDOS in VirtualBox though.

## Bitmap fonts
Rendering text these days is really, really hard. When we go zooming around documents or web pages via mouse wheel or touch zoom, we expect text to scale seamlessly and stay crisp. If we want to get fancy, we add [kerning](https://en.wikipedia.org/wiki/Kerning) and [hinting](https://en.wikipedia.org/wiki/Font_hinting) to the mix.

It gets even harder when non-latin scripts like [arabic script](https://en.wikipedia.org/wiki/Arabic_script) or [CJK script](https://en.wikipedia.org/wiki/CJK_characters) need to get put on a screen. Now you have to deal with [ligatures](https://fonts.google.com/knowledge/glossary/ligature), mixed left-to-right and [right-to-left layouting](https://en.wikipedia.org/wiki/Right-to-left_script), and various other nightmares.

And to top it all off, what you get out of a font file is usually a vector representation of not a character, but a [glyph](https://en.wikipedia.org/wiki/Glyph), which can be a character, or a part of a character, and oh my, this is all very complicated.

Thankfully, there are various libraries that can help us draw text. For translating a text string to a set of glyphs, or [shaping](https://harfbuzz.github.io/what-is-harfbuzz.html#what-is-text-shaping) as it's usually called, you can use [HarfBuzz](https://harfbuzz.github.io/). If you want to rasterize those glyphs, which are usually given in vector form, you can use [FreeType](http://freetype.org/). If you  want to use your GPU to do most of that, you can use [Slug](https://sluglibrary.com/). Your operating system usually also comes with [APIs to draw text](https://learn.microsoft.com/en-us/windows/win32/directwrite/direct-write-portal).

We aren't going to do any of that though. We'll be going somewhat old school and draw inspiration from [VGA text mode fonts](https://en.wikipedia.org/wiki/VGA_text_mode#Fonts), but with a 2022 spirit (aka being wasteful).

But before we can look at font pixels, we need to talk about character encodings.

### Character encodings
Text is composed of characters. When we store text digitally, those characters need to be stored as a sequence of (binary) numbers. When we read characters from a file to draw them to the screen, or translate key strokes to characters, we need to map numbers back to characters. Similarly, when the C compiler encounters a string literal like `const char *text = "Hello world"`, it will convert the characters in the string to a sequence of numbers that gets embedded in the final executable.

Mapping those sequences of numbers to characters and vice versa is what [character encodings](https://en.wikipedia.org/wiki/Character_encoding) are for.

One of the oldest character encodings is [ASCII](https://en.wikipedia.org/wiki/ASCII). Each character is encoded in 1 byte. Well, actually, ASCII only uses the first 7-bits, so it encodes a total of 128 characters. Well, that's not quite true either. Only 95 of these characters are printable. The other 33 "characters" are what's called [control codes](https://en.wikipedia.org/wiki/Control_character). Notable ones are `\t` or `9`, which indicates a tab, and `\n` or `10`, the line feed.

Here are all the printable characters and non-printable control codes contained in ASCII with their (hexa-)decimal codes.

--markdown-end
{{post.code("", "none", `
> ascii -d
Dec Hex    Dec Hex    Dec Hex  Dec Hex  Dec Hex  Dec Hex   Dec Hex   Dec Hex
  0 00 NUL  16 10 DLE  32 20    48 30 0  64 40 @  80 50 P   96 60 \`  112 70 p
  1 01 SOH  17 11 DC1  33 21 !  49 31 1  65 41 A  81 51 Q   97 61 a  113 71 q
  2 02 STX  18 12 DC2  34 22 "  50 32 2  66 42 B  82 52 R   98 62 b  114 72 r
  3 03 ETX  19 13 DC3  35 23 #  51 33 3  67 43 C  83 53 S   99 63 c  115 73 s
  4 04 EOT  20 14 DC4  36 24 $  52 34 4  68 44 D  84 54 T  100 64 d  116 74 t
  5 05 ENQ  21 15 NAK  37 25 %  53 35 5  69 45 E  85 55 U  101 65 e  117 75 u
  6 06 ACK  22 16 SYN  38 26 &  54 36 6  70 46 F  86 56 V  102 66 f  118 76 v
  7 07 BEL  23 17 ETB  39 27 '  55 37 7  71 47 G  87 57 W  103 67 g  119 77 w
  8 08 BS   24 18 CAN  40 28 (  56 38 8  72 48 H  88 58 X  104 68 h  120 78 x
  9 09 HT   25 19 EM   41 29 )  57 39 9  73 49 I  89 59 Y  105 69 i  121 79 y
 10 0A LF   26 1A SUB  42 2A *  58 3A :  74 4A J  90 5A Z  106 6A j  122 7A z
 11 0B VT   27 1B ESC  43 2B +  59 3B ;  75 4B K  91 5B [  107 6B k  123 7B {
 12 0C FF   28 1C FS   44 2C ,  60 3C <  76 4C L  92 5C \  108 6C l  124 7C |
 13 0D CR   29 1D GS   45 2D -  61 3D =  77 4D M  93 5D ]  109 6D m  125 7D }
 14 0E SO   30 1E RS   46 2E .  62 3E >  78 4E N  94 5E ^  110 6E n  126 7E ~
 15 0F SI   31 1F US   47 2F /  63 3F ?  79 4F O  95 5F _  111 6F o  127 7F DEL
`)}}
--markdown-begin

The codes `0-31` are control codes, including the `\t` (`9`) and `\n` (`10`) codes we discussed above. Printable characters start at code 32 (` ` or space) and go to code `126`. The final code `127` is another control code.

ASCII is short for "American Standard Code for Information Interchange". Unsurprisingly, the ASCII encoding really only contains characters used in US English, and by coincidence, some other western scripts.

Now, I'm not 'merican. And based on my server logs, chances are good you aren't 'merican either. What about other fancy characters, like 'ö' or 'ê'? Or characters from the arabic or CJK scripts? Well, that's a lot more complicated and historically involves something called [code pages](https://en.wikipedia.org/wiki/Code_page), which was and still is an utter mess.

The alternative to code pages is [Unicode](https://en.wikipedia.org/wiki/Unicode), which has multiple encodings, like [UTF-8](https://en.wikipedia.org/wiki/UTF-8), [UTF-16](https://en.wikipedia.org/wiki/UTF-16), and so on. Thankfully, the world has now mostly standardized on UTF-8, for [good reasons](http://utf8everywhere.org/).

UTF-8 is a multi-byte encoding. Depending on the character, we may need 1 to 4 bytes to store it. That covers pretty much all the characters used around the world, including all those annoying emojis your older family members send you instead of actual words. Thanks Unicode.

For our demos, we'll store text either in C source code as literals ala `const char *text = "Hello world"`, or in text files in the `assets/` folder of the `r96` project. Both the C sources and text files will be encoded using `UTF-8`. Anything else would be pain. This means we have to deal with UTF-8 when rendering text.

But as I said earlier, we do not want to go full Unicode text rendering, as that'd require us to integrate all the fancy libraries mentioned above. We want a simpler solution. Enter Unicode's first 256 code points. These code points are split up into 2 blocks.

The first block from code point `0-127` is called the [Basic Latin Unicode block](https://en.wikipedia.org/wiki/Basic_Latin_(Unicode_block)). The code points are the exact same codes as used in ASCII, including both non-printable control codes (`0-31` and `127`) and printable characters (`32-126`). When encoding text with UTF-8, the resulting sequence of bytes is backwards compatible with ASCII: the first 128 Unicode code points get encoded as a single byte in UTF-8.

The second block from code point `128-255` is called the [Latin 1 Supplement block](https://en.wikipedia.org/wiki/Latin-1_Supplement). It contains another set of non-printable control codes (`128-159`) called [C1 controls](https://en.wikipedia.org/wiki/C0_and_C1_control_codes#C1_controls), which we can safely ignore for the purpose of rendering text. The remaining code points in the block (`160-255`) include additional characters used in some western scripts. These Unicode code points are encoded with 2 bytes in UTF-8.

Surprise! Those first 256 Unicode code points map directly onto an old code page, namely, the  [ISO-8859-1 character set](https://en.wikipedia.org/wiki/ISO/IEC_8859-1). It is sometimes incorrectly referred to as extended ASCII. Here are the characters contained in the set.

--markdown-end
{{post.figureMaxWidth("iso-8859-1.png", "The ISO-8859-1 character set <a href='https://en.wikipedia.org/wiki/ISO/IEC_8859-1'>Source: Wikipedia</a>", "80%")}}
--markdown-begin

E.g. `ö` is encoded as `0xF6` or `246` in decimal. The gray blocks are the control codes.

Alright, we've decided to use the first 2 Unicode blocks spanning code points `0-255`. All our C source code containing string literals will be stored UTF-8 encoded. Any any text files we put into `assets/` to be read by our demos will also be UTF-8 encoded. There are two minor complications.

The first complication is how C compilers handle string literals. When the compiler encounters something like `const char *text = "Hello world"`, it will use a character encoding to turn the literal `"Hello world"` into a sequence of bytes embedded in the executable. Which encoding is chosen, depends on the compiler. By default, Clang and GCC convert the string literal to UTF-8 and embed the corresponding byte sequence. Clang even assumes that the source file encoding is UTF-8 and refuses to compile anything else. [MSVC is ... different](https://pspdfkit.com/blog/2021/string-literals-character-encodings-and-multiplatform-cpp/). Luckily, we do not care for MSVC in this series. If you do care for some reason, just make sure to pass `/utf8` as a compiler flag to ensure MSVC embeds string literals as UTF-8 as well.

The second complication is actually reading the code points of a UTF-8 encoded text string, whether it comes from a C string literal or a UTF-8 encoded file read from disk. We have to deal with the multi-byte nature of the UTF-8 encoding, as code points above `127` are encoded as two bytes. Luckily, I've taken care of that with the function [`r96_next_utf8_character()`](https://github.com/badlogic/r96/blob/04-dos-nostalgia/src/r96/r96.c#L15-L31):

--markdown-end
{{post.code("r96.c", "c", `
uint32_t r96_next_utf8_codepoint(const char *data, uint32_t *index, uint32_t end) {
	static const uint32_t utf8_offsets[6] = {
			0x00000000UL, 0x00003080UL, 0x000E2080UL,
			0x03C82080UL, 0xFA082080UL, 0x82082080UL};

	uint32_t character = 0;
	const unsigned char *bytes = (const unsigned char *) data;
	int num_bytes = 0;
	do {
		character <<= 6;
		character += bytes[(*index)++];
		num_bytes++;
	} while (*index != end && ((bytes[*index]) & 0xC0) == 0x80);
	character -= utf8_offsets[num_bytes - 1];

	return character;
}
`)}}
--markdown-begin

This function takes a sequence of bytes (`data`) encoding a UTF-8 string, an index into the byte sequence, and the last valid index (`end`). Both indices are byte offsets, not character offsets!

The function then reads the next UTF-8 character, which may be 1 to 4 bytes long, and returns its code point. Additionally, it increments the `index` accordingly, so we know at what byte offset the next character starts.

> **Note:** I stole the original of this function many years ago from ... somewhere. I can not remember anymore. I've since modified it to my needs. To the original author: I'm deeply sorry I forgot who you are.

We can use this function to iterate all UTF-8 characters in a byte sequence and get their code points:

--markdown-end
{{post.code("", "c", `
const char *utf8_text = "¡ÄÖ$\n\t";
uint32_t index = 0;
uint32_t end = strlen(utf8_text);
while (index != end) {
	uint32_t code_point = r96_next_utf8_codepoint(utf8_text, &index, end);
	printf("code point: %i/%x\n", code_point, code_point);
}
`)}}
--markdown-begin

Which prints the code point of each character in decimal and hexadecimal.

```
code point: 161/a1
code point: 196/c4
code point: 214/d6
code point: 36/24
code point: 10/a
code point: 9/9
```

As expected. Compare the output to the ISO-8859-1 chart above for validation.

This function can deal with any valid UTF-8 byte sequence and returns code points as a 32-bit unsigned integer. For our purposes, we are only interested in code points `0-255` and will ignore any other code points.

## Generating a glyph atlas
Alright, we have all our encoding bases covered. The next question is: how do we turn a code point like `64` (`0x41`) into the corresponding glyph image for the character `A` so we can blit it onto the screen?



## Next time on "Mario writes a lot of words"

Discuss this post on [Twitter]() or [Mastodon]().

--markdown-end
<script>
tableOfContents()
</script>

{{include "../../../_templates/post_footer.bt.html"}}