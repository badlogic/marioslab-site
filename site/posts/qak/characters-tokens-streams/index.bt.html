{{
metadata = {
	title: "Qak - Of characters, tokens, and streams",
	summary: "",
	image: "token.jpeg",
	date: parseDate("2020/07/12 23:00"),
	published: true,
}
}}
<!-- Hi! -->

{{include "../../../_templates/post_header.bt.html"}}
{{include "../../../_templates/post_header.bt.html" as post}}

{{post.figure("token.jpeg", "Not the token we're looking for.")}}

<h2>Qak - Of characters, tokens, and streams</h2>

<p>
	Last time, I laid out <a href="https://marioslab.io/posts/qak/minimally-viable-product/">the plan for Qak 0.1</a>, the minimally viable product version of the language. Quite a bit of work has been completed already, and many adventures, especially related to performance improvements, have been made. Great material for another blargh post. Today we'll have a look at the first step of making sense of a Qak source file though. A bit boring, but it needs to be done.
</p>

<p>
	What's this magical first step? It goes by many names: <a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexical analysis, lexing, scanning, or tokenization</a> and probably a gazillion more. If you had any kind of exposure to formal computer science education, you've likely heard one or more of these terms.
</p>

<p>
	In non-academic words, we want to take a bunch of bytes from a source like a source code file, figure out which bytes make up a character, then group characters together into "words", also known as <strong>tokens</strong>, that are valid for the programming language. Going forward, I shall refer to this process as <strong>tokenization</strong>.
</p>

<p>
	Here's a little Qak snippet to illustrate tokenization:
</p>

<pre><code>// variable declaration with initializers and simple type inference
// Variables without initializer will be initialized to the type's
// default value.
var foo = 123
var bar: boolean = true
var zeroInitializer: int32

// While statement, who needs for(-each)?!
while(bar)
	// Variables are block scoped
	var uff = 3
end
</code></pre>

<p>
	This source code consists of the tokens <code>var</code>, <code>foo</code>, <code>=</code>, <code>123</code>, <code>var</code>, <code>bar</code>, <code>:</code>, <code>boolean</code>, <code>=</code>, <code>true</code>, <code>var</code>, <code>zeroInitializer</code>, <code>:</code>, <code>int32</code>, <code>while</code>, <code>(</code>, <code>bar</code>, <code>)</code>, <code>var</code>, <code>uff</code>, <code>=</code>, <code>3</code>, and <code>end</code>.
</p>

<p>During tokenization, we really don't care if the tokens in that order make sense within the rules of the programming language. All we care about is that they are valid "words" within the programming language.</p>

<p>You might be wondering why the comments starting with <code>//</code> aren't tokens, or what happens with whitespace. The comments could actually be tokens themselves, which might come in handy if we wanted to implement something like JavaDocs or Doxygen. At this point, I won't do that, so comments get ignored by the tokenization process. Whitespace of any form is also ignored, as Qak is not a whitespace sensitive language like Python.</p>

<p class="blockquote">
	Funnily enough, a smart guy called <a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy">Chomsky's</a> came up with a bunch of sciency stuff to describe (formal) grammars of natural languages. And while initially promising, Chomsky's work failed to be applicable to natural languages, which led to many sad trombone sounds in the linguist community. At least that's what my linguist wife told me. I'm obliged by law to believe her.</br></br>

	Chomsky's work was actually quite useful for us computer folks. Many of the techniques we use, like <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions</a>, or their recursive version, <a href="https://en.wikipedia.org/wiki/Parsing">parsers</a>, can be fitted Chomsky's formal grammars. They are also a fantastic way to torture students in their first compiler engineering university course.
</p>

<p>
	I'll have none of that academic rigorosity here, nor will I use any of the available generators for tokenizers. Instead, I'll go with handcrafted, artisan, organic, and possibly buggy and slow tokenization of my own design. Here we go.
</p>

<h3>Qak compiler would like to know your (source) location</h3>

<p>
	The Qak compiler will need to get source code from somewhere. Could be a file. Could be a network stream. Or a string embedded in your program that calls into the Qak compiler. What the compiler cares about is the bytes making up the source code, and a name for nicely reporting errors in that source.
</p>

<p>
	In the Qak code, the <a href="https://github.com/badlogic/qak/blob/master/src/source.h#L41"><code>Source</code></a> struct encapsulates an array of bytes of a fixed length, paired with a (file) name making up a source code "file". It can also give us inidividual lines delimited by <code>\n</code> in the source code, which comes in handy when we print out nice errors with line highlights later on. Here's the pseudo-code version of that struct, with details omitted:
<p>

<pre><code>struct Source {
	const char *fileName;
	uint8_t *data;
	size_t size;

	Array&lt;Line&gt; &lines();
}
</code></pre>

<p>
	The compiler will later refer to parts of that source, e.g. the location of a token within, or the characters spanned by an entire  <code>while</code> statement and its body. The compiler needs this information for two tasks:
</p>

<ul>
	<li>Getting substrings for "things", e.g. the text of a token like <code>123</code> to turn it into a number, or the name of a variable to look up. This can be expressed as a start and end byte-offset into the source data.</li>
	<li>Error reporting. Here we traditionally want lines numbers.</li>
</ul>

<p>In the Qak source code, a location in a source is stored as a <a href="">https://github.com/badlogic/qak/blob/master/src/source.h#L99<code>Span</code></a>, which looks like this in pseudo code:</p>

<pre><code>struct Span {
	Source &amp;source;
	uint32_t start;
	uint32_t end;
	uint32_t startLine;
	uint32_t endLine;
}</code></pre>

<p>
Pretty straight forward. The <code>end</code> offset is exclusive, <code>startLine</code> and <code>endLine</code> are indices into the array returned by <code>Source::lines()</code> and start at 1 instead of 0.
</p>

<p>
	Speaking of lines, Qak also has a representation for those expressed via the <a href="https://github.com/badlogic/qak/blob/master/src/source.h#L17"><code>Line</code></a> struct. Here's the pseudo code:
</p>

<pre><code>struct Line {
	uint32_t start;
	uint32_t end;
	uint32_t lineNumber;
}
</code></pre>

<p>
	<code>start</code> and <code>end</code> are again byte-offsets into the source's data.
</p>

<p>
	Alright, we can now store source data, and refer to parts of the source data in byte-offset and line-based ways. Let's have a look at how we can get individual characters from the source data. Turns out that's a bit more involved than just taking one byte at a time if we want those sweet, sweet emojis in our source code ðŸ¤¡ ðŸ’©.
</p>


<h3>Plowing through UTF-8 characters</h3>
<p>
	What's a character? This question has tortured us computer-y folks for decades. We still kind of suck at it, but somehow agree that <a href="https://en.wikipedia.org/wiki/UTF-8#:~:text=UTF%2D8%20(8%2Dbit,Ken%20Thompson%20and%20Rob%20Pike.">UTF-8</a> is the least sucky way to approach that question. All of this is really boring, so here's code to traverse a stream of bytes one UTF-8 character at a time, which may be composed of 1, 2, 3, or 4 sequential bytes. It's really all magic to me.
</p>

<pre><code>/* Reads the next UTF-8 character from the stream and returns it as a UTF-32 character.
* The index is updated to point to the next UTF-8 character in the stream.
* Taken from https://www.cprogramming.com/tutorial/utf8.c */
static QAK_FORCE_INLINE uint32_t nextUtf8Character(const uint8_t *data, uint32_t *index) {
	static const uint32_t utf8Offsets[6] = {
			0x00000000UL, 0x00003080UL, 0x000E2080UL,
			0x03C82080UL, 0xFA082080UL, 0x82082080UL
	};

	uint32_t character = 0;
	int sz = 0;
	do {
		character <<= 6;
		character += data[(*index)++];
		sz++;
	} while (data[*index] && !(((data[*index]) & 0xC0) != 0x80));
	character -= utf8Offsets[sz - 1];

	return character;
}</code></pre>

<p>
	The code is adapted from this fine <a href="https://www.cprogramming.com/tutorial/unicode.html">article</a> by Jeff Bezanson, whom I thank for figuring this out instead of me having to do it.
</p>

<p>
	The function takes bytes making up UTF-8 characters and an byte-index into the bytes, reads the next UTF-8 character, updates the index by the number of bytes read, and returns the character as UTF-32. Repeatedly calling this function allows us to traverse all UTF-8 characters.
</p>

</p>

<h2>Up next</h2>
<p>This should be enough to create a contrived factorial nano benchmark to compare Qak against the likes of Lua and Python. Next time we'll look at the implementation of the parser and generation of the abstract syntax tree for the poor excuse of a language specification outlined above.</p>

<p>Discuss this post by replying to <a href="https://twitter.com/badlogicgames/status/1276218596943953922">this tweet.</a></p>

{{include "../../../_templates/post_footer.bt.html"}}